{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "import google_auth_oauthlib\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the default `print()` function with one that only prints if the global variable `VERBOSE` is set to `True`.  \n",
    "The `'_print' in globals()` check is used to allow re-execution of the cell without messing up the function pointers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = True\n",
    "if '_print' in globals(): print = _print\n",
    "_print = print\n",
    "def print(*args, force_print=False, **kwargs):\n",
    "    if VERBOSE or force_print: _print(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_playlist_data(data):\n",
    "    # TODO have more than one file to write to, such that the last few versions are all kept, not just the last one.\n",
    "    json.dump(data, open('playlist_data.json', 'w'))\n",
    "    \n",
    "def load_playlist_data():\n",
    "    try: return json.load(open('playlist_data.json'))\n",
    "    except FileNotFoundError:\n",
    "        print('playlist_data.json doesn\\'t exist yet, using empty dict.')\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose whether you need to use the API Key to authenticate, or OAuth 2.0.  \n",
    "If you only deal with public playlists, then the API Key is enough.  \n",
    "If you want to backup your own private playlists, then you need to use OAuth 2.0.  \n",
    "OAuth 2.0 strictly supersedes the API Key in terms of permissions. Generation, and using it is a bit more tedious though, so it is recommended to use the API Key if you don't need the extended permissions of OAuth 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key:str = json.load(open('api_key.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OAuth 2.0 Client IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret:dict        = json.load(open('client_secret.json'))\n",
    "client_id:str      = secret['installed']['client_id']\n",
    "client_secret:str  = secret['installed']['client_secret']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to re-authenticate every time you want to run this program. Even if you store the credentials, there is a timeout on googles side that prevents you from re-using old credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = google_auth_oauthlib.get_user_credentials(\n",
    "    client_id=client_id, client_secret=client_secret,\n",
    "    scopes='https://www.googleapis.com/auth/youtube.readonly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the credentials to disk in order to not have to re-authenticate when you're just restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_credentials(credentials): pickle.dump(credentials, open('credentials.pkl', 'wb'))\n",
    "def load_credentials()           : return pickle.load(open('credentials.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_credentials(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = load_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query all playlists belonging to a channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_playlists_from_channel(channel_id:str=None) -> list:\n",
    "    \"Recieve metadata about all playlists belonging to the youtube channel with `channel_id`.\\n\"\\\n",
    "    \"If no `channel_id` is given, and OAuth 2.0 was used to authenticate, \"\\\n",
    "    \"then the authenticated users channel will be used.\"\n",
    "    if channel_id:\n",
    "        request = youtube.playlists().list(part='snippet', maxResults=50, channelId=channel_id)\n",
    "    else:\n",
    "        request = youtube.playlists().list(part='snippet', maxResults=50, mine=True)\n",
    "    playlists = []\n",
    "    while request is not None:\n",
    "        response = request.execute(num_retries=2)\n",
    "        playlists.extend(response['items'])\n",
    "        request = youtube.playlists().list_next(request, response)\n",
    "    return [(pl['id'], pl['snippet']['title'], pl['snippet']['channelTitle'], pl) for pl in playlists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlists = all_playlists_from_channel('UC38IQsAvIsxxjztdMZQtwHA')\n",
    "len(playlists), len(playlists[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kind', 'etag', 'id', 'snippet'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlists[0][-1].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all Playlist metadata for all channels and playlists set in tracked_ids.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playlist_info(playlist_id:str) -> dict:\n",
    "    \"Retrieve metadata for the playlist with given `playlist_id`.\"\n",
    "    request = youtube.playlists().list(part='snippet', id=playlist_id)\n",
    "    response = request.execute()\n",
    "    assert response['pageInfo']['totalResults'] == 1, f'There should exist exactly one playlist per ID. {response}'\n",
    "    pl = response['items'][0]\n",
    "    return pl['id'], pl['snippet']['title'], pl['snippet']['channelTitle'], pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_playlists_from_file():\n",
    "    \"Loads channel and playlist ids from the `tracked_ids.json` file, \"\\\n",
    "    \"and retrieves metadata for all associated playlists.\"\n",
    "    file = json.load(open('tracked_ids.json'))\n",
    "    playlist_ids = file['playlists']\n",
    "    playlists = [playlist_info(p_id) for p_id in playlist_ids]\n",
    "    channel_ids = file['channels']\n",
    "    for c_id in channel_ids:\n",
    "        playlists.extend(all_playlists_from_channel(c_id))\n",
    "    return playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_playlists_from_file())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query all information about a playlists items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_playlist_items(playlist_id:str) -> list:\n",
    "    \"Revieve a list of metadata for all videos in the playlist with given `playlist_id`.\"\n",
    "    request = youtube.playlistItems().list(part='snippet', playlistId=playlist_id, maxResults=50)\n",
    "    playlist_items = []\n",
    "    while request is not None:\n",
    "        response = request.execute(num_retries=2)\n",
    "        playlist_items.extend(response['items'])\n",
    "        request = youtube.playlistItems().list_next(request, response)\n",
    "    return playlist_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = all_playlist_items('PL2MI040U_GXq1L5JUxNOulWCyXn-7QyZK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kind', 'etag', 'id', 'snippet'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/watch?v=dQw4w9WgXcQ\n"
     ]
    }
   ],
   "source": [
    "print('https://www.youtube.com/watch?v=' + items[0]['snippet']['resourceId']['videoId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(playlists:list, remove_missing_videos:bool=False, remove_missing_playlists:bool=False):\n",
    "    old_pl_data       :dict = load_playlist_data()\n",
    "    new_pl_data       :dict = dict()\n",
    "    plist_contained   :set  = set()\n",
    "    any_change_to_data:bool = False\n",
    "    \n",
    "    for plist_id, title, channel_name, plist in playlists:\n",
    "        print(f'-------------------------------------------------------')\n",
    "        print(f'-------------------------{title} @ {channel_name}')\n",
    "        \n",
    "        # NOTE: Check if this playlist was already present in previous runs.\n",
    "        # Then add the data that's stored in the playlist dict about the videos\n",
    "        # to the new version of the playlist dict.\n",
    "        playlist_known:bool = (plist_id in old_pl_data)\n",
    "        if playlist_known:\n",
    "            # TODO: Check for changed in playlist metadata?\n",
    "            # Don't really care about that, but might be useful...\n",
    "            plist['videos'] = old_pl_data[plist_id]['videos']\n",
    "        else:\n",
    "            print(f'New Playlist with ID \"{plist_id}\" | Titled \"{title}\"')\n",
    "            plist['videos'] = {}\n",
    "        new_pl_data[plist_id] = plist\n",
    "\n",
    "        # NOTE: Check for duplicate playlists. This should actually never happen, but you never know...\n",
    "        if plist_id in plist_contained:\n",
    "            print(f'Duplicate Playlist ID \"{plist_id}\"')\n",
    "        plist_contained.add(plist_id)\n",
    "\n",
    "        videos     :dict = plist['videos']\n",
    "        contained  :set  = set()\n",
    "        plist_has_changed = False\n",
    "        plist_has_new     = False\n",
    "        \n",
    "        # NOTE: deleted videos won't even show up in this list,\n",
    "        # so we have to check against saved info from previous runs later using `contained` and `videos`.\n",
    "        for item in all_playlist_items(plist_id):\n",
    "            video_id = item['snippet']['resourceId']['videoId']\n",
    "\n",
    "            # NOTE: Check if this video is set to Private.\n",
    "            is_private = ((not item['snippet']['thumbnails'])\n",
    "                           or (item['snippet']['title']       == 'Private video')\n",
    "                           or (item['snippet']['description'] == 'This video is private.'))\n",
    "            if is_private:\n",
    "                print(f'Private Video with ID \"{video_id}\" at position {item[\"snippet\"][\"position\"]}')\n",
    "\n",
    "            # NOTE: Check if this video has already appeared before in this playlist.\n",
    "            # This is unaffected by data from previous runs.\n",
    "            is_duplicate = (video_id in contained)\n",
    "            if is_duplicate:\n",
    "                print(f'Duplicate Video ID \"{video_id}\" at position {item[\"snippet\"][\"position\"]}. '\\\n",
    "                      f'Previous occurrence is at position {videos[video_id][\"snippet\"][\"position\"]}')\n",
    "            contained.add(video_id)\n",
    "\n",
    "            # NOTE: Check if this video is known from previous runs or not.\n",
    "            # Also check the videos title or description has changed compared to the previous run.\n",
    "            # This will in almost every case also detect when a video has been set to private since the last run.\n",
    "            # Only exception is if the original video was titiled \"Private video\",\n",
    "            # and had the description \"This video is private.\"... That should never happen in practice though,\n",
    "            # and at that point it doesn't really matter anyway, since the user will still know which video it was.\n",
    "            is_new = not (video_id in videos)\n",
    "            if is_new:\n",
    "                plist_has_new = True\n",
    "                print(f'New Video with ID \"{video_id}\" detected.')\n",
    "            else:\n",
    "                old = videos[video_id]\n",
    "                \n",
    "                old_title, new_title = old['snippet']['title'], item['snippet']['title']\n",
    "                if (old_title != new_title):\n",
    "                    plist_has_changed = True\n",
    "                    print(f'Video with ID \"{video_id}\"\\'s Title has changed.\\n'\\\n",
    "                          f'\\t\"{old_title}\" -> \"{new_title}\"')\n",
    "                    \n",
    "                old_description, new_description = old['snippet']['description'], item['snippet']['description']\n",
    "                if (old_description != new_description):\n",
    "                    plist_has_changed = True\n",
    "                    print(f'Video with ID \"{video_id}\"\\'s Description has changed.\\n'\\\n",
    "                          f'\\tLength: {len(old_description)} -> {len(new_description)}')\n",
    "            \n",
    "            videos[video_id] = item # This overwrites whatever video data was there before with the new data.\n",
    "        \n",
    "        # NOTE: Check and Inform user about changes to Tracked Data.\n",
    "        any_change_to_data = (any_change_to_data or (plist_has_new or plist_has_changed))\n",
    "        if plist_has_new:     print('New Videos have been added to Tracked Data.')\n",
    "        if plist_has_changed: print('Changes to Titles and / or Descriptions have been applied.')\n",
    "\n",
    "        # NOTE: Check if any videos that have appeared in previous runs are no longer present.\n",
    "        missing = set(videos.keys()).difference(contained)\n",
    "        if len(missing) > 0:\n",
    "            print('Missing Videos detected!')\n",
    "            if remove_missing_videos:\n",
    "                any_change_to_data = True\n",
    "                print('Following Videos have been removed from Tracked Data:')\n",
    "            else:\n",
    "                print('Following Videos are missing from received data, but will be carried over from backup:')\n",
    "            for id in missing:\n",
    "                print(f'\\tID: {id} (pos: {videos[id][\"snippet\"][\"position\"]})\\t'\\\n",
    "                      f'| Old Title: {videos[id][\"snippet\"][\"title\"]}')\n",
    "                if remove_missing_videos: videos.pop(id)\n",
    "                    \n",
    "\n",
    "    # NOTE: Check if any playlists that have appeared in previous runs are no longer present.\n",
    "    plist_missing = set(old_pl_data.keys()).difference(plist_contained)\n",
    "    if len(plist_missing) > 0:\n",
    "        print(f'Missing Playlist detected!')\n",
    "        if remove_missing_playlists:\n",
    "            any_change_to_data = True\n",
    "            print('Following Playlists have been removed from Tracked Data:')\n",
    "        else:\n",
    "            print('Following Playlists are missing from received data, but will be carried over from backup:')\n",
    "        for id in plist_missing:\n",
    "            print(f'\\tID: {id} | Old Title: {old_pl_data[id][\"snippet\"][\"title\"]}')\n",
    "            if not remove_missing_playlists: new_pl_data[id] = old_pl_data[id]\n",
    "\n",
    "    print('\\nTo remove missing videos or playlist from Tracked Data, pass the corresponding flags to main().')\n",
    "    \n",
    "    print('='*75, force_print=True)\n",
    "    if any_change_to_data: print('!---CAUTION---! Tracked Data has Changed !---CAUTION---!', force_print=True)\n",
    "    else:                  print('No Change to Tracked Data.', force_print=True)\n",
    "    \n",
    "    return old_pl_data, new_pl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlists = load_playlists_from_file()\n",
    "len(playlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`main()` loads the previous backup, queries all videos for all playlists that are passed as the argument, and compares that information with the backup. It then returns the backup data and the newly constructed date for further analysis, or to overwrite the old backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "-------------------------Ted Nelson's Computers for Cynics @ bennokr\n",
      "-------------------------------------------------------\n",
      "-------------------------AlphaGo, AlphaZero, AlphaGo Zero @ Two Minute Papers\n",
      "-------------------------------------------------------\n",
      "-------------------------3D Printing / 3D Fabrication @ Two Minute Papers\n",
      "-------------------------------------------------------\n",
      "-------------------------Light Transport, Ray Tracing and Global Illumination (Two Minute Papers) @ Two Minute Papers\n",
      "-------------------------------------------------------\n",
      "-------------------------Fluid, Cloth and Hair Simulations (Two Minute Papers) @ Two Minute Papers\n",
      "Duplicate Video ID \"CSQPD3oyvD8\" at position 15. Previous occurrence is at position 14\n",
      "Duplicate Video ID \"CSQPD3oyvD8\" at position 16. Previous occurrence is at position 15\n",
      "-------------------------------------------------------\n",
      "-------------------------AI and Deep Learning - Two Minute Papers @ Two Minute Papers\n",
      "Duplicate Video ID \"_BPJFFkxSbw\" at position 231. Previous occurrence is at position 230\n",
      "Private Video with ID \"XQEDw6IKTK8\" at position 309\n",
      "Private Video with ID \"hBobYd8nNtQ\" at position 310\n",
      "Private Video with ID \"pqkpIfu36Os\" at position 311\n",
      "Private Video with ID \"_lphFvojA-8\" at position 312\n",
      "Private Video with ID \"vYLnvBpXUfA\" at position 313\n",
      "-------------------------------------------------------\n",
      "-------------------------LuxRender @ Two Minute Papers\n",
      "-------------------------------------------------------\n",
      "-------------------------Two Minute Papers @ Two Minute Papers\n",
      "Private Video with ID \"pqkpIfu36Os\" at position 173\n",
      "Private Video with ID \"hBobYd8nNtQ\" at position 174\n",
      "Private Video with ID \"p-u2NZ4vUxI\" at position 175\n",
      "Private Video with ID \"_lphFvojA-8\" at position 301\n",
      "Private Video with ID \"vYLnvBpXUfA\" at position 303\n",
      "Private Video with ID \"XQEDw6IKTK8\" at position 391\n",
      "-------------------------------------------------------\n",
      "-------------------------TU Wien Rendering / Ray Tracing Course @ Two Minute Papers\n",
      "-------------------------------------------------------\n",
      "-------------------------Mostly Science Videos @ Two Minute Papers\n",
      "Private Video with ID \"P0Tkr4HaIVk\" at position 5\n",
      "\n",
      "To remove missing videos or playlist from Tracked Data, pass the corresponding flags to main().\n",
      "===========================================================================\n",
      "No Change to Tracked Data.\n"
     ]
    }
   ],
   "source": [
    "old_pl_data, new_pl_data = main(playlists, remove_missing_videos=True, remove_missing_playlists=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_playlist_data(new_pl_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Stored Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of Data:\n",
    "{\n",
    "    \"random_playlist_id_1\" : {\n",
    "        \"id\" : \"random_playlist_id_1\",\n",
    "        \"snippet\" : {\n",
    "            \"channelId\" : \"random_id_of_channel_that_owns_this_playlist\",\n",
    "            \"title\" : \"The Title of this playlist\",\n",
    "            \"description\" : \"The Description of this playlist\",\n",
    "            \"channelTitle\" : \"Title of the channel that owns this playlist\"\n",
    "        },\n",
    "        \"videos\" : {\n",
    "            \"random_video_id_1\" : {\n",
    "                \"snippet\" : {\n",
    "                    \"channelId\" : \"random_id_of_channel_that_owns_this_playlist\",\n",
    "                    \"title\" : \"The Title of this video\",\n",
    "                    \"description\" : \"The Description of this video\",\n",
    "                    \"channelTitle\" : \"Title of the channel that owns this playlist\",\n",
    "                    \"position\" : 123,\n",
    "                    \"resourceId\" : {\"videoId\" : \"random_id_of_this_video\"}\n",
    "                }\n",
    "            },\n",
    "            \"random_video_id_2\" : {},\n",
    "            \"random_video_id_3\" : {},\n",
    "        }\n",
    "    },\n",
    "    \"random_playlist_id_2\" : {},\n",
    "    \"random_playlist_id_3\" : {},\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_playlist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid, plist = data.popitem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kind', 'etag', 'id', 'snippet', 'videos'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid, video = plist['videos'].popitem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['publishedAt', 'channelId', 'title', 'description', 'thumbnails', 'channelTitle', 'playlistId', 'position', 'resourceId'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video['snippet'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
